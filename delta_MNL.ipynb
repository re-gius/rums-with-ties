{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# delta-MNLs with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"data/mnl_datasets\"\n",
    "train_folder = \"train\"\n",
    "test_folder = \"test\"\n",
    "rumwt_folder = \"data/rumwt_pred\"\n",
    "scores_folder = \"data/slates\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sushi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sushi_train = \"sushi_10_4310_3_1000_0.5_train.csv\"\n",
    "ds_sushi_test = \"sushi_10_431_3_10000_0.5_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sushi_train_df = pd.read_csv(f\"{base_folder}/{train_folder}/{ds_sushi_train}\")\n",
    "# Look at the first 5 rows of the data\n",
    "sushi_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sushi_test_df = pd.read_csv(f\"{base_folder}/{test_folder}/{ds_sushi_test}\")\n",
    "# Look at the first 5 rows of the data\n",
    "sushi_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numpy = sushi_train_df.drop(['slate_ID','no-choice','CHOICE'], axis=1).values\n",
    "X_test_numpy = sushi_test_df.drop(['slate_ID','no-choice','CHOICE'], axis=1).values\n",
    "\n",
    "num_classes = X_train_numpy.shape[1] + 1\n",
    "y_train_numpy = np.subtract(sushi_train_df.CHOICE.values,1)\n",
    "y_test_numpy = np.subtract(sushi_test_df.CHOICE.values,1)\n",
    "\n",
    "X_train = torch.tensor(X_train_numpy, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_numpy)\n",
    "X_test = torch.tensor(X_test_numpy, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(vector, n_classes):\n",
    "    one_hot = torch.zeros((vector.shape[0], n_classes))\\\n",
    "        .type(torch.LongTensor)  # 1\n",
    "    return one_hot.scatter(\n",
    "        1, vector.type(torch.LongTensor).unsqueeze(1), 1\n",
    "    )\n",
    "\n",
    "\n",
    "y_train_one_hot = one_hot_encode(y_train,num_classes)\n",
    "y_test_one_hot = one_hot_encode(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_one_hot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_softmax_activation(z,delta):\n",
    "    z = torch.sub(z, torch.max(z).repeat(z.size()))\n",
    "    exponentials = torch.exp(z)  # exp(z_ij)\n",
    "    shifted_exponentials = torch.exp(torch.add(z,torch.abs(delta))) # exp(z_ij + delta)\n",
    "    shifted_exponentials_sums = torch.sum(shifted_exponentials, axis=1).unsqueeze(1).repeat(1,shifted_exponentials.size(axis=1))\n",
    "    exponentials_norm = shifted_exponentials_sums - shifted_exponentials + exponentials;\n",
    "    probabilities = exponentials / exponentials_norm\n",
    "    relu = torch.nn.ReLU()\n",
    "    missing_probs = relu(torch.sub(torch.ones(probabilities.size(axis=0),1),torch.sum(probabilities, axis=1).unsqueeze(1)))\n",
    "    return torch.cat((probabilities, missing_probs), 1)\n",
    "\n",
    "def cross_entropy_loss(y_one_hot, activations):\n",
    "    return -torch.mean(torch.sum(y_one_hot * torch.log(activations), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = X_train_numpy.shape[1]\n",
    "w_autograd = torch.rand((num_dims,num_dims), requires_grad=True)\n",
    "b_autograd = torch.rand(num_dims, requires_grad=True)\n",
    "delta_autograd = torch.rand(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_iterations = 10000\n",
    "learning_rate = 0.1\n",
    "lambda_param = 0.0005\n",
    "DELTA = 1.1\n",
    "for i in range(1, n_iterations + 1):\n",
    "    \n",
    "    Z = torch.mm(X_train, w_autograd) + b_autograd\n",
    "    A = delta_softmax_activation(Z,delta_autograd)\n",
    "    l2_regularization = torch.sum(w_autograd ** 2)\n",
    "    loss = cross_entropy_loss(y_train_one_hot, A) \\\n",
    "           + lambda_param * l2_regularization\n",
    "    \n",
    "    if w_autograd.grad is not None:\n",
    "        w_autograd.grad.zero_()\n",
    "    if b_autograd.grad is not None:\n",
    "        b_autograd.grad.zero_()\n",
    "    if delta_autograd.grad is not None:\n",
    "        delta_autograd.grad.zero_()\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w_autograd -= learning_rate * w_autograd.grad\n",
    "        b_autograd -= learning_rate * b_autograd.grad\n",
    "        delta_autograd -= learning_rate * delta_autograd.grad\n",
    "    \n",
    "    if i == 1 or i % 100 == 0:\n",
    "        print(delta_autograd)\n",
    "        print(\"Loss at iteration {}: {}\".format(i, loss))\n",
    "        print(\"Non-regularized Loss at iteration {}: {}\".format(i, loss - lambda_param * l2_regularization))\n",
    "\n",
    "    # print(delta_softmax_activation(torch.mm(X_test, w_autograd) + b_autograd, delta_autograd))\n",
    "test_predictions = torch.argmax(\n",
    "    delta_softmax_activation(torch.mm(X_test, w_autograd) + b_autograd, torch.tensor(delta_autograd)), axis=1\n",
    ")\n",
    "test_accuracy = float(sum(test_predictions == y_test)) / y_test.shape[0]\n",
    "print(\"\\nFinal Test Accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_winner_probs_test = 'sushi_10_431_3_10000_0.5_winner_probs_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rumwt_probs_test = np.genfromtxt(f'{rumwt_folder}/{ds_winner_probs_test}', delimiter=',')\n",
    "print(y_rumwt_probs_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dmnl_probs_test = delta_softmax_activation(torch.mm(X_test, w_autograd) + b_autograd, delta_autograd).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_dmnl_probs_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_var(y, y_pred):\n",
    "    losses = []\n",
    "    num_items = y_pred.shape[1]\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        loss=np.linalg(y,y_pred)\n",
    "        losses.append(loss)\n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, y_pred):\n",
    "    losses = []\n",
    "    num_items = y_pred.shape[1]\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        loss=-np.sum(y[i]*np.log(y_pred[i]+0.000001))/num_items  # +eps for numerical stability\n",
    "        losses.append(loss)\n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div(y, y_pred):\n",
    "    losses = []\n",
    "    num_items = y_pred.shape[1]\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        loss=(-np.sum(y[i]*np.log(y_pred[i]+0.000001))+np.sum(y[i]*np.log(y[i]+0.000001)))/num_items # +eps for numerical stability\n",
    "        losses.append(loss)\n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = np.linalg.norm(y_rumwt_probs_test-y_dmnl_probs_test,ord=1,axis=1)\n",
    "total_variation = np.amax(l1)\n",
    "mean_variation = np.mean(l1)\n",
    "median_variation = np.median(l1)\n",
    "print(total_variation)\n",
    "print(mean_variation)\n",
    "print(median_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = kl_div(y_rumwt_probs_test,y_dmnl_probs_test)\n",
    "total_ce = np.amax(kl)\n",
    "mean_ce = np.mean(kl)\n",
    "median_ce = np.median(kl)\n",
    "print(total_ce)\n",
    "print(mean_ce)\n",
    "print(median_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_char(c):\n",
    "    if c == '-':\n",
    "        return 0\n",
    "    return float(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_test_scores, delta=0.5):\n",
    "    score = 0.0\n",
    "    for i in range(len(y_pred)):\n",
    "        top_index = np.argmax(y_pred[i][:-1])\n",
    "        if y_test_scores[i][top_index] > max(y_test_scores[i]) - delta:\n",
    "            score += 1\n",
    "    return score/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sushi_test_scores_df = pd.read_csv(f\"{scores_folder}/{test_folder}/sushi_10_431_3_10000_test.csv\")\n",
    "# Look at the first 5 rows of the data\n",
    "sushi_test_scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scores_str = sushi_test_scores_df.to_numpy()\n",
    "y_test_scores = []\n",
    "for row in y_test_scores_str:\n",
    "    y_test_scores.append([cast_char(x) for x in row])\n",
    "print(y_test_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(y_dmnl_probs_test, y_test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Young People Spending Habits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ypsh_train = \"young_people_spendinghabits_7_1010_2_1000_0.5_train.csv\"\n",
    "ds_ypsh_test = \"young_people_spendinghabits_7_101_2_10000_0.5_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypsh_train_df = pd.read_csv(f\"{base_folder}/{train_folder}/{ds_ypsh_train}\")\n",
    "# Look at the first 5 rows of the data\n",
    "ypsh_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ypsh_test_df = pd.read_csv(f\"{base_folder}/{test_folder}/{ds_ypsh_test}\")\n",
    "# Look at the first 5 rows of the data\n",
    "ypsh_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numpy = ypsh_train_df.drop(['slate_ID','no-choice','CHOICE'], axis=1).values\n",
    "X_test_numpy = ypsh_test_df.drop(['slate_ID','no-choice','CHOICE'], axis=1).values\n",
    "\n",
    "num_classes = X_train_numpy.shape[1] + 1\n",
    "y_train_numpy = np.subtract(ypsh_train_df.CHOICE.values,1)\n",
    "y_test_numpy = np.subtract(ypsh_test_df.CHOICE.values,1)\n",
    "\n",
    "X_train = torch.tensor(X_train_numpy, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_numpy)\n",
    "X_test = torch.tensor(X_test_numpy, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(vector, n_classes):\n",
    "    one_hot = torch.zeros((vector.shape[0], n_classes))\\\n",
    "        .type(torch.LongTensor)  # 1\n",
    "    return one_hot.scatter(\n",
    "        1, vector.type(torch.LongTensor).unsqueeze(1), 1\n",
    "    )\n",
    "\n",
    "\n",
    "y_train_one_hot = one_hot_encode(y_train,num_classes)\n",
    "y_test_one_hot = one_hot_encode(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_one_hot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = X_train_numpy.shape[1]\n",
    "w_autograd = torch.rand((num_dims,num_dims), requires_grad=True)\n",
    "b_autograd = torch.rand(num_dims, requires_grad=True)\n",
    "delta_autograd = torch.rand(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_iterations = 10000\n",
    "learning_rate = 0.5\n",
    "lambda_param = 0.0001\n",
    "DELTA = 1.1\n",
    "for i in range(1, n_iterations + 1):\n",
    "    \n",
    "    Z = torch.mm(X_train, w_autograd) + b_autograd\n",
    "    A = delta_softmax_activation(Z,delta_autograd)\n",
    "    l2_regularization = torch.sum(w_autograd ** 2)\n",
    "    loss = cross_entropy_loss(y_train_one_hot, A) \\\n",
    "           + lambda_param * l2_regularization\n",
    "    \n",
    "    if w_autograd.grad is not None:\n",
    "        w_autograd.grad.zero_()\n",
    "    if b_autograd.grad is not None:\n",
    "        b_autograd.grad.zero_()\n",
    "    if delta_autograd.grad is not None:\n",
    "        delta_autograd.grad.zero_()\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w_autograd -= learning_rate * w_autograd.grad\n",
    "        b_autograd -= learning_rate * b_autograd.grad\n",
    "        delta_autograd -= learning_rate * delta_autograd.grad\n",
    "    \n",
    "    if i == 1 or i % 100 == 0:\n",
    "        print(delta_autograd)\n",
    "        print(\"Loss at iteration {}: {}\".format(i, loss))\n",
    "        print(\"Non-regularized Loss at iteration {}: {}\".format(i, loss - lambda_param * l2_regularization))\n",
    "\n",
    "    # print(delta_softmax_activation(torch.mm(X_test, w_autograd) + b_autograd, delta_autograd))\n",
    "test_predictions = torch.argmax(\n",
    "    delta_softmax_activation(torch.mm(X_test, w_autograd) + b_autograd, torch.tensor(delta_autograd)), axis=1\n",
    ")\n",
    "test_accuracy = float(sum(test_predictions == y_test)) / y_test.shape[0]\n",
    "print(\"\\nFinal Test Accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_winner_probs_test = 'young_people_spendinghabits_7_101_2_10000_0.5_winner_probs_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rumwt_probs_test = np.genfromtxt(f'{rumwt_folder}/{ds_winner_probs_test}', delimiter=',')\n",
    "print(y_rumwt_probs_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dmnl_probs_test = delta_softmax_activation(torch.mm(X_test, w_autograd) + b_autograd, delta_autograd).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_dmnl_probs_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = np.linalg.norm(y_rumwt_probs_test-y_dmnl_probs_test,ord=1,axis=1)\n",
    "total_variation = np.amax(l1)\n",
    "mean_variation = np.mean(l1)\n",
    "median_variation = np.median(l1)\n",
    "print(total_variation)\n",
    "print(mean_variation)\n",
    "print(median_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kl = kl_div(y_rumwt_probs_test,y_dmnl_probs_test)\n",
    "total_ce = np.amax(kl)\n",
    "mean_ce = np.mean(kl)\n",
    "median_ce = np.median(kl)\n",
    "print(total_ce)\n",
    "print(mean_ce)\n",
    "print(median_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypsh_test_scores_df = pd.read_csv(f\"{scores_folder}/{test_folder}/young_people_spendinghabits_7_101_2_10000_test.csv\")\n",
    "# Look at the first 5 rows of the data\n",
    "ypsh_test_scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scores_str = ypsh_test_scores_df.to_numpy()\n",
    "y_test_scores = []\n",
    "for row in y_test_scores_str:\n",
    "    y_test_scores.append([cast_char(x) for x in row])\n",
    "print(y_test_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(y_dmnl_probs_test, y_test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TripAdvisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_trip_train = \"tripadvisor_10_980_4_10000_0.5_train.csv\"\n",
    "ds_trip_test = \"tripadvisor_10_98_4_10000_0.5_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_train_df = pd.read_csv(f\"{base_folder}/{train_folder}/{ds_trip_train}\")\n",
    "# Look at the first 5 rows of the data\n",
    "trip_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trip_test_df = pd.read_csv(f\"{base_folder}/{test_folder}/{ds_trip_test}\")\n",
    "# Look at the first 5 rows of the data\n",
    "trip_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numpy = trip_train_df.drop(['slate_ID','no-choice','CHOICE'], axis=1).values\n",
    "X_test_numpy = trip_test_df.drop(['slate_ID','no-choice','CHOICE'], axis=1).values\n",
    "\n",
    "num_classes = X_train_numpy.shape[1] + 1\n",
    "y_train_numpy = np.subtract(trip_train_df.CHOICE.values,1)\n",
    "y_test_numpy = np.subtract(trip_test_df.CHOICE.values,1)\n",
    "\n",
    "X_train = torch.tensor(X_train_numpy, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_numpy)\n",
    "X_test = torch.tensor(X_test_numpy, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(vector, n_classes):\n",
    "    one_hot = torch.zeros((vector.shape[0], n_classes))\\\n",
    "        .type(torch.LongTensor)  # 1\n",
    "    return one_hot.scatter(\n",
    "        1, vector.type(torch.LongTensor).unsqueeze(1), 1\n",
    "    )\n",
    "\n",
    "\n",
    "y_train_one_hot = one_hot_encode(y_train,num_classes)\n",
    "y_test_one_hot = one_hot_encode(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_one_hot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = X_train_numpy.shape[1]\n",
    "w_autograd = torch.rand((num_dims,num_dims), requires_grad=True)\n",
    "b_autograd = torch.rand(num_dims, requires_grad=True)\n",
    "delta_autograd = torch.rand(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_iterations = 10000\n",
    "learning_rate = 0.5\n",
    "lambda_param = 0.0001\n",
    "DELTA = 1.1\n",
    "for i in range(1, n_iterations + 1):\n",
    "    \n",
    "    Z = torch.mm(X_train, w_autograd) + b_autograd\n",
    "    A = delta_softmax_activation(Z,delta_autograd)\n",
    "    l2_regularization = torch.sum(w_autograd ** 2)\n",
    "    loss = cross_entropy_loss(y_train_one_hot, A) \\\n",
    "           + lambda_param * l2_regularization\n",
    "    \n",
    "    if w_autograd.grad is not None:\n",
    "        w_autograd.grad.zero_()\n",
    "    if b_autograd.grad is not None:\n",
    "        b_autograd.grad.zero_()\n",
    "    if delta_autograd.grad is not None:\n",
    "        delta_autograd.grad.zero_()\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w_autograd -= learning_rate * w_autograd.grad\n",
    "        b_autograd -= learning_rate * b_autograd.grad\n",
    "        delta_autograd -= learning_rate * delta_autograd.grad\n",
    "    \n",
    "    if i == 1 or i % 100 == 0:\n",
    "        print(delta_autograd)\n",
    "        print(\"Loss at iteration {}: {}\".format(i, loss))\n",
    "        print(\"Non-regularized Loss at iteration {}: {}\".format(i, loss - lambda_param * l2_regularization))\n",
    "\n",
    "    # print(delta_softmax_activation(torch.mm(X_test, w_autograd) + b_autograd, delta_autograd))\n",
    "test_predictions = torch.argmax(\n",
    "    delta_softmax_activation(torch.mm(X_test, w_autograd) + b_autograd, torch.tensor(delta_autograd)), axis=1\n",
    ")\n",
    "test_accuracy = float(sum(test_predictions == y_test)) / y_test.shape[0]\n",
    "print(\"\\nFinal Test Accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_winner_probs_test = 'tripadvisor_10_98_4_10000_0.5_winner_probs_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rumwt_probs_test = np.genfromtxt(f'{rumwt_folder}/{ds_winner_probs_test}', delimiter=',')\n",
    "print(y_rumwt_probs_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dmnl_probs_test = delta_softmax_activation(torch.mm(X_test, w_autograd) + b_autograd, delta_autograd).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_dmnl_probs_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = np.linalg.norm(y_rumwt_probs_test-y_dmnl_probs_test,ord=1,axis=1)\n",
    "total_variation = np.amax(l1)\n",
    "mean_variation = np.mean(l1)\n",
    "median_variation = np.median(l1)\n",
    "print(total_variation)\n",
    "print(mean_variation)\n",
    "print(median_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = kl_div(y_rumwt_probs_test,y_dmnl_probs_test)\n",
    "total_ce = np.amax(kl)\n",
    "mean_ce = np.mean(kl)\n",
    "median_ce = np.median(kl)\n",
    "print(total_ce)\n",
    "print(mean_ce)\n",
    "print(median_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_test_scores_df = pd.read_csv(f\"{scores_folder}/{test_folder}/tripadvisor_10_98_4_10000_test.csv\")\n",
    "# Look at the first 5 rows of the data\n",
    "trip_test_scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scores_str = trip_test_scores_df.to_numpy()\n",
    "y_test_scores = []\n",
    "for row in y_test_scores_str:\n",
    "    y_test_scores.append([cast_char(x) for x in row])\n",
    "print(y_test_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(y_dmnl_probs_test, y_test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_movie_train = \"movies_20_174130_5_100000_0.25_train.csv\"\n",
    "ds_movie_test = \"movies_20_17413_5_10000_0.25_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_train_df = pd.read_csv(f\"{base_folder}/{train_folder}/{ds_movie_train}\")\n",
    "# Look at the first 5 rows of the data\n",
    "movie_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_test_df = pd.read_csv(f\"{base_folder}/{test_folder}/{ds_movie_test}\")\n",
    "# Look at the first 5 rows of the data\n",
    "movie_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numpy = movie_train_df.drop(['slate_ID','no-choice','CHOICE'], axis=1).values\n",
    "X_test_numpy = movie_test_df.drop(['slate_ID','no-choice','CHOICE'], axis=1).values\n",
    "\n",
    "num_classes = X_train_numpy.shape[1] + 1\n",
    "y_train_numpy = np.subtract(movie_train_df.CHOICE.values,1)\n",
    "y_test_numpy = np.subtract(movie_test_df.CHOICE.values,1)\n",
    "\n",
    "X_train = torch.tensor(X_train_numpy, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_numpy)\n",
    "X_test = torch.tensor(X_test_numpy, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(vector, n_classes):\n",
    "    one_hot = torch.zeros((vector.shape[0], n_classes))\\\n",
    "        .type(torch.LongTensor)  # 1\n",
    "    return one_hot.scatter(\n",
    "        1, vector.type(torch.LongTensor).unsqueeze(1), 1\n",
    "    )\n",
    "\n",
    "\n",
    "y_train_one_hot = one_hot_encode(y_train,num_classes)\n",
    "y_test_one_hot = one_hot_encode(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_one_hot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = X_train_numpy.shape[1]\n",
    "w_autograd = torch.rand((num_dims,num_dims), requires_grad=True)\n",
    "b_autograd = torch.rand(num_dims, requires_grad=True)\n",
    "delta_autograd = torch.rand(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_iterations = 10000\n",
    "learning_rate = 0.1\n",
    "lambda_param = 0.0001\n",
    "DELTA = 1.1\n",
    "for i in range(1, n_iterations + 1):\n",
    "    \n",
    "    Z = torch.mm(X_train, w_autograd) + b_autograd\n",
    "    A = delta_softmax_activation(Z,delta_autograd)\n",
    "    l2_regularization = torch.sum(w_autograd ** 2)\n",
    "    loss = cross_entropy_loss(y_train_one_hot, A) \\\n",
    "           + lambda_param * l2_regularization\n",
    "    \n",
    "    if w_autograd.grad is not None:\n",
    "        w_autograd.grad.zero_()\n",
    "    if b_autograd.grad is not None:\n",
    "        b_autograd.grad.zero_()\n",
    "    if delta_autograd.grad is not None:\n",
    "        delta_autograd.grad.zero_()\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w_autograd -= learning_rate * w_autograd.grad\n",
    "        b_autograd -= learning_rate * b_autograd.grad\n",
    "        delta_autograd -= learning_rate * delta_autograd.grad\n",
    "    \n",
    "    if i == 1 or i % 100 == 0:\n",
    "        print(delta_autograd)\n",
    "        print(\"Loss at iteration {}: {}\".format(i, loss))\n",
    "        print(\"Non-regularized Loss at iteration {}: {}\".format(i, loss - lambda_param * l2_regularization))\n",
    "\n",
    "    # print(delta_softmax_activation(torch.mm(X_test, w_autograd) + b_autograd, delta_autograd))\n",
    "test_predictions = torch.argmax(\n",
    "    delta_softmax_activation(torch.mm(X_test, w_autograd) + b_autograd, torch.tensor(delta_autograd)), axis=1\n",
    ")\n",
    "test_accuracy = float(sum(test_predictions == y_test)) / y_test.shape[0]\n",
    "print(\"\\nFinal Test Accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_winner_probs_test = 'movies_20_17413_5_10000_0.25_winner_probs_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rumwt_probs_test = np.genfromtxt(f'{rumwt_folder}/{ds_winner_probs_test}', delimiter=',')\n",
    "print(y_rumwt_probs_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dmnl_probs_test = delta_softmax_activation(torch.mm(X_test, w_autograd) + b_autograd, delta_autograd).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_dmnl_probs_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = np.linalg.norm(y_rumwt_probs_test-y_dmnl_probs_test,ord=1,axis=1)\n",
    "total_variation = np.amax(l1)\n",
    "mean_variation = np.mean(l1)\n",
    "median_variation = np.median(l1)\n",
    "print(total_variation)\n",
    "print(mean_variation)\n",
    "print(median_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = kl_div(y_rumwt_probs_test,y_dmnl_probs_test)\n",
    "total_ce = np.amax(kl)\n",
    "mean_ce = np.mean(kl)\n",
    "median_ce = np.median(kl)\n",
    "print(total_ce)\n",
    "print(mean_ce)\n",
    "print(median_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_test_scores, delta=0.25):\n",
    "    score = 0.0\n",
    "    for i in range(len(y_pred)):\n",
    "        top_index = np.argmax(y_pred[i][:-1])\n",
    "        if y_test_scores[i][top_index] > max(y_test_scores[i]) - delta:\n",
    "            score += 1\n",
    "    return score/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_test_scores_df = pd.read_csv(f\"{scores_folder}/{test_folder}/movies_20_17413_5_10000_test.csv\")\n",
    "# Look at the first 5 rows of the data\n",
    "movie_test_scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scores_str = movie_test_scores_df.to_numpy()\n",
    "y_test_scores = []\n",
    "for row in y_test_scores_str:\n",
    "    y_test_scores.append([cast_char(x) for x in row])\n",
    "print(y_test_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(y_dmnl_probs_test, y_test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## goodBooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_book_train = \"books_30_47211_5_1000000_0.5_train.csv\"\n",
    "ds_book_test = \"books_30_4721_5_10000_0.5_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_train_df = pd.read_csv(f\"{base_folder}/{train_folder}/{ds_book_train}\")\n",
    "# Look at the first 5 rows of the data\n",
    "book_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_test_df = pd.read_csv(f\"{base_folder}/{test_folder}/{ds_book_test}\")\n",
    "# Look at the first 5 rows of the data\n",
    "book_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numpy = book_train_df.drop(['slate_ID','no-choice','CHOICE'], axis=1).values\n",
    "X_test_numpy = book_test_df.drop(['slate_ID','no-choice','CHOICE'], axis=1).values\n",
    "\n",
    "num_classes = X_train_numpy.shape[1] + 1\n",
    "y_train_numpy = np.subtract(book_train_df.CHOICE.values,1)\n",
    "y_test_numpy = np.subtract(book_test_df.CHOICE.values,1)\n",
    "\n",
    "X_train = torch.tensor(X_train_numpy, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_numpy)\n",
    "X_test = torch.tensor(X_test_numpy, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(vector, n_classes):\n",
    "    one_hot = torch.zeros((vector.shape[0], n_classes))\\\n",
    "        .type(torch.LongTensor)  # 1\n",
    "    return one_hot.scatter(\n",
    "        1, vector.type(torch.LongTensor).unsqueeze(1), 1\n",
    "    )\n",
    "\n",
    "\n",
    "y_train_one_hot = one_hot_encode(y_train,num_classes)\n",
    "y_test_one_hot = one_hot_encode(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_one_hot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = X_train_numpy.shape[1]\n",
    "w_autograd = torch.rand((num_dims,num_dims), requires_grad=True)\n",
    "b_autograd = torch.rand(num_dims, requires_grad=True)\n",
    "delta_autograd = torch.rand(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_iterations = 10000\n",
    "learning_rate = 0.1\n",
    "lambda_param = 0.0001\n",
    "DELTA = 1.1\n",
    "for i in range(1, n_iterations + 1):\n",
    "    \n",
    "    Z = torch.mm(X_train, w_autograd) + b_autograd\n",
    "    A = delta_softmax_activation(Z,delta_autograd)\n",
    "    l2_regularization = torch.sum(w_autograd ** 2)\n",
    "    loss = cross_entropy_loss(y_train_one_hot, A) \\\n",
    "           + lambda_param * l2_regularization\n",
    "    \n",
    "    if w_autograd.grad is not None:\n",
    "        w_autograd.grad.zero_()\n",
    "    if b_autograd.grad is not None:\n",
    "        b_autograd.grad.zero_()\n",
    "    if delta_autograd.grad is not None:\n",
    "        delta_autograd.grad.zero_()\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w_autograd -= learning_rate * w_autograd.grad\n",
    "        b_autograd -= learning_rate * b_autograd.grad\n",
    "        delta_autograd -= learning_rate * delta_autograd.grad\n",
    "    \n",
    "    if i == 1 or i % 100 == 0:\n",
    "        print(delta_autograd)\n",
    "        print(\"Loss at iteration {}: {}\".format(i, loss))\n",
    "        print(\"Non-regularized Loss at iteration {}: {}\".format(i, loss - lambda_param * l2_regularization))\n",
    "\n",
    "    # print(delta_softmax_activation(torch.mm(X_test, w_autograd) + b_autograd, delta_autograd))\n",
    "test_predictions = torch.argmax(\n",
    "    delta_softmax_activation(torch.mm(X_test, w_autograd) + b_autograd, torch.tensor(delta_autograd)), axis=1\n",
    ")\n",
    "test_accuracy = float(sum(test_predictions == y_test)) / y_test.shape[0]\n",
    "print(\"\\nFinal Test Accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_winner_probs_test = 'books_30_4721_5_10000_0.5_winner_probs_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rumwt_probs_test = np.genfromtxt(f'{rumwt_folder}/{ds_winner_probs_test}', delimiter=',')\n",
    "print(y_rumwt_probs_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dmnl_probs_test = delta_softmax_activation(torch.mm(X_test, w_autograd) + b_autograd, delta_autograd).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_dmnl_probs_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = np.linalg.norm(y_rumwt_probs_test-y_dmnl_probs_test,ord=1,axis=1)\n",
    "total_variation = np.amax(l1)\n",
    "mean_variation = np.mean(l1)\n",
    "median_variation = np.median(l1)\n",
    "print(total_variation)\n",
    "print(mean_variation)\n",
    "print(median_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = kl_div(y_rumwt_probs_test,y_dmnl_probs_test)\n",
    "total_ce = np.amax(kl)\n",
    "mean_ce = np.mean(kl)\n",
    "median_ce = np.median(kl)\n",
    "print(total_ce)\n",
    "print(mean_ce)\n",
    "print(median_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
